# -*- coding: utf-8 -*-
"""Data_Generation_LLM_Diffuser_CLIP_20250913 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i09kvtP24rpILjszP_QvTXKye6qMmx_Q

# Text to Image Generation
"""

pip install git+https://github.com/openai/CLIP.git

from google.colab import drive
# Mount Google Drive\n",
drive.mount('/content/drive')

from diffusers import StableDiffusionPipeline
import torch
import clip
from PIL import Image
import matplotlib.pyplot as plt

# -------------------------------
# Load Stable Diffusion (CPU or GPU)
# -------------------------------
model_id = "CompVis/stable-diffusion-v1-4"
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float32 if device=="cpu" else torch.float16
)
pipe = pipe.to(device)
if device == "cpu":
    pipe.enable_attention_slicing()

# -------------------------------
# Load CLIP model
# -------------------------------
model, preprocess = clip.load("ViT-B/32", device=device)

import os
from diffusers import StableDiffusionPipeline
import torch
import clip
from PIL import Image
import matplotlib.pyplot as plt

# -------------------------------
# CIFAR-10 class names
# -------------------------------
cifar_classes = [
    "airplane", "automobile", "bird", "cat", "deer",
    "dog", "frog", "horse", "ship", "truck"
]

# -------------------------------
# airplane_descriptions dict: {idx: prompt}
# -------------------------------
# For testing, just take first 10 items
prompts = {k: v for k, v in list(descriptions.items())[:]}

# -------------------------------
# Reference text embeddings for CLIP
# -------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"

# Use simple template: "a photo of a {cls}" for text reference
cls_template_prompts = [f"a photo of a {cls}" for cls in cifar_classes]

# Tokenize template prompts
text_tokens = clip.tokenize(cls_template_prompts).to(device)

# Encode CLIP text embeddings
with torch.no_grad():
    text_features = model.encode_text(text_tokens)
    text_features /= text_features.norm(dim=-1, keepdim=True)  # normalize

# -------------------------------
# Output path
# -------------------------------
output_path = "/content/drive/MyDrive/Synthetic_Image/"
os.makedirs(output_path, exist_ok=True)

# -------------------------------
# Generate + Infer + Save
# -------------------------------
def generate_and_infer(prompts_dict, expected_class, thresh=0.9):
    results = []
    failed_descriptions = []
    failed_prompts = {}

    for idx, p in prompts_dict.items():  # <-- use dict keys as idx
        # Generate image
        generator = torch.manual_seed(42 + idx)
        image = pipe(
            p, guidance_scale=7.5, num_inference_steps=20, generator=generator
        ).images[0]

        # CLIP inference
        image_input = preprocess(image).unsqueeze(0).to(device)
        with torch.no_grad():
            image_features = model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            logits_per_image = 100.0 * image_features @ text_features.T
            probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]

        # Determine top-1 prediction
        top_class = cifar_classes[probs.argmax()]
        top_conf = float(probs.max())

        # Check alignment and confidence
        aligned = (top_class == expected_class)
        high_conf = (top_conf >= thresh)

        if aligned and high_conf:
            # Save image
            class_folder = os.path.join(output_path, expected_class)
            os.makedirs(class_folder, exist_ok=True)
            img_path = os.path.join(class_folder, f"{expected_class}_{idx}.png")
            image.save(img_path)
            status = f"✅ Aligned & confident (confidence {top_conf:.2f}) - Saved to {img_path}"
        else:
            # Determine reason
            if not aligned:
                reason = "not aligned"
            elif not high_conf:
                reason = f"low confidence {top_conf:.2f}"
            else:
                reason = "unknown"

            status = (
                f"❌ Failed. Reason: {reason}. "
                f"Prompt: {p} -> Predicted '{top_class}'"
            )

            failed_descriptions.append({
                "idx": idx,
                "prompt": p,
                "expected_class": expected_class,
                "predicted_class": top_class,
                "confidence": top_conf,
                "soft_labels": {cls: float(pr) for cls, pr in zip(cifar_classes, probs)},
                "status": status
            })

            # Add to failed_prompts dict
            failed_prompts[idx] = p

        print(f"{idx} - {status}")

        results.append({
            "idx": idx,
            "prompt": p,
            "expected_class": expected_class,
            "predicted_class": top_class,
            "confidence": top_conf,
            "soft_labels": {cls: float(pr) for cls, pr in zip(cifar_classes, probs)},
            "status": status
        })

    return results, failed_descriptions, failed_prompts


# -------------------------------
# Run
# -------------------------------
results, failed, failed_prompts = generate_and_infer(prompts, expected_class="automobile", thresh=0.95)

print("\nFailed Descriptions:")
for f in failed:
    print(f"- {f['idx']} -> {f['status']} and soft label is: {f['soft_labels']}")

print("\nFailed Prompts Dict:")
print(failed_prompts)

# -------------------------------
# Edited prompts dictionary
# -------------------------------
edited_prompts = {
    11: "A photo of a red sedan automobile with doors, windows, and rear trunk fully visible.",
    13: "A close-up of a blue sports car showing headlights, shiny grille, and sleek side mirrors.",
    21: "A centered image of a black SUV automobile showing headlights, front grille, and cabin clearly.",
    30: "A detailed photo of a silver coupe automobile with round headlights, tires, and smooth body visible.",
    36: "A clear photo of a white hatchback automobile with smooth hood, bumper, and side panels visible.",
    41: "A photo of a small green compact car with all four tires, doors, and windows visible.",
    53: "A centered image of a yellow sedan automobile showing tires, doors, and headlights clearly.",
    56: "A crisp capture of a gray sports car automobile showing headlights, tires, and roofline.",
    59: "A clear photo of a red convertible car with headlights, grille, and tires fully visible.",
    73: "A detailed image of a blue sedan automobile showing headlights, front bumper, and aerodynamic body.",
    75: "A crisp photo of a black hatchback car showing tires, rear trunk, and headlights.",
    80: "A wide capture of a silver SUV automobile with roof, hood, and front bumper visible.",
    81: "A crisp image of a white sedan car showing full cabin, doors, and windows.",
    88: "A crisp photo of a red coupe automobile with trunk, doors, and roof clearly visible.",
    91: "A bright photo of a green sedan automobile showing full side, roof, and trunk panels.",
    93: "A wide photo of a gray hatchback car showing tires, doors, and roofline clearly.",
    94: "A crisp centered image of a black sports car showing headlights, grille, and hood.",
    96: "A bright shot of a blue sedan automobile with doors, tires, and headlights fully visible.",
    102: "A centered photo of a silver SUV automobile showing tires, doors, and roof fully visible.",
    104: "A detailed shot of a white sedan automobile with doors, trunk, and front bumper visible.",
    114: "A centered photo of a red hatchback car showing headlights, doors, and tires clearly."
}



# -------------------------------
# Run with only edited prompts
# -------------------------------
results, failed, failed_prompts = generate_and_infer(
    edited_prompts,
    expected_class="automobile",
    thresh=0.95
)

print("\nFailed Descriptions:")
for f in failed:
    print(f"- {f['idx']} -> {f['status']} and soft label is: {f['soft_labels']}")

print("\nFailed Prompts Dict:")
print(failed_prompts)